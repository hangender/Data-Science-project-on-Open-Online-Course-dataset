{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(327686, 59) (327686,)\n",
      "(161397, 59) (161397,)\n",
      "learning rate: 0.01 neurons/layer: 40 accuracy: 0.965006 f1: 0.00738137075678\n",
      "learning rate: 0.1 neurons/layer: 40 accuracy: 0.984287 f1: 0.768444120884\n",
      "learning rate: 0.3 neurons/layer: 40 accuracy: 0.98443 f1: 0.763437747955\n",
      "learning rate: 0.01 neurons/layer: 31 accuracy: 0.966635 f1: 0.100851558149\n",
      "learning rate: 0.1 neurons/layer: 31 accuracy: 0.984628 f1: 0.771610081196\n",
      "learning rate: 0.3 neurons/layer: 31 accuracy: 0.984944 f1: 0.782219111919\n",
      "CPU times: user 4.31 s, sys: 13.8 s, total: 18.1 s\n",
      "Wall time: 57.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pylab\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "moocs = pd.read_csv('~/MOOC_CleanedDataSet.csv',index_col=0)\n",
    "\n",
    "moocs_sample = moocs.sample(frac=0.67, replace=False) #sample frac%\n",
    "\n",
    "moocs_label = np.asarray(pd.DataFrame(moocs_sample, columns = ['Grades_Category']))\n",
    "moocs_label = moocs_label.flatten()\n",
    "\n",
    "#del moocs['Video']\n",
    "#del moocs['Forums']\n",
    "del moocs_sample['Grades_Category']\n",
    "\n",
    "moocs_train = np.asarray(moocs_sample)\n",
    "\n",
    "print (str(moocs_train.shape) + \" \" + str(moocs_label.shape))\n",
    "\n",
    "moocs_sample1 = moocs.drop(moocs_sample.index) #create the test set\n",
    "\n",
    "moocs_label1 = np.asarray(pd.DataFrame(moocs_sample1, columns = ['Grades_Category']))\n",
    "moocs_label1 = moocs_label1.flatten()\n",
    "\n",
    "#del moocs['Video']\n",
    "#del moocs['Forums']\n",
    "del moocs_sample1['Grades_Category']\n",
    "\n",
    "moocs_test = np.asarray(moocs_sample1)\n",
    "\n",
    "print (str(moocs_test.shape) + \" \" + str(moocs_label1.shape))\n",
    "\n",
    "#moocs_train/moocs_label, moocs_test/moocs_label1\n",
    "rdd_train = sc.broadcast(moocs_train)\n",
    "rdd_train_label = sc.broadcast(moocs_label)\n",
    "\n",
    "rdd_test = sc.broadcast(moocs_test)\n",
    "rdd_test_label = sc.broadcast(moocs_label1)\n",
    "\n",
    "#NN model defined here\n",
    "def map_fun(arr):\n",
    "\n",
    "    learning_rate_ = arr[0]\n",
    "    neurons_ = arr[1]\n",
    "    import tensorflow as tf\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import pylab\n",
    "    import pandas as pd\n",
    "    import random\n",
    "\n",
    "    moocs_train = rdd_train.value\n",
    "    moocs_label = rdd_train_label.value\n",
    "    moocs_test = rdd_test.value\n",
    "    moocs_label1 = rdd_test_label.value\n",
    "\n",
    "    # Parameters\n",
    "    learning_rate = learning_rate_\n",
    "    num_steps = 800 #each step takes in 1 batch_size\n",
    "    batch_size = 128 #size of each training iteration\n",
    "    \n",
    "    print ('number of passes: ' + str((num_steps*batch_size)/moocs_train.shape[0]))\n",
    "    display_step = 100 #not used\n",
    "\n",
    "    # Network Parameters\n",
    "    #the optimal size of the hidden layer is usually between the size of the input and size of the output layers\n",
    "    n_hidden_1 = neurons_ # 1st layer number of neurons\n",
    "    n_hidden_2 = neurons_ # 2nd layer number of neurons\n",
    "    num_input = moocs_train.shape[1] # number of features per input\n",
    "    num_classes = 2 # predict 0/1 ~bad grade/good grade (>=60%)\n",
    "\n",
    "    # Define the input function for training\n",
    "    input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'images': moocs_train}, y=moocs_label,\n",
    "        batch_size=batch_size, num_epochs=None, shuffle=True)\n",
    "\n",
    "    # Define the neural network\n",
    "    def neural_net(x_dict):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict['images']\n",
    "        # Hidden fully connected layer with 256 neurons\n",
    "        layer_1 = tf.layers.dense(x, n_hidden_1)\n",
    "        # Hidden fully connected layer with 256 neurons\n",
    "        layer_2 = tf.layers.dense(layer_1, n_hidden_2)\n",
    "        # Output fully connected layer with a neuron for each class\n",
    "        out_layer = tf.layers.dense(layer_2, num_classes)\n",
    "        return out_layer\n",
    "\n",
    "    # Define the model function (following TF Estimator Template)\n",
    "    def model_fn(features, labels, mode):\n",
    "\n",
    "        # Build the neural network\n",
    "        logits = neural_net(features)\n",
    "\n",
    "        # Predictions\n",
    "        pred_classes = tf.argmax(logits, axis=1)\n",
    "        pred_probas = tf.nn.softmax(logits)\n",
    "\n",
    "        # If prediction mode, early return\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
    "\n",
    "        # Define loss and optimizer\n",
    "        loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "        #tf.nn.softmax_cross_entropy_with_logits for floats\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "\n",
    "        # Evaluate the accuracy of the model\n",
    "        acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes) #number of rights/total\n",
    "        recall = tf.metrics.recall(labels=labels, predictions=pred_classes)\n",
    "        precision = tf.metrics.precision(labels=labels, predictions=pred_classes)\n",
    "\n",
    "        # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "        # the different ops for training, evaluating, ...\n",
    "        estim_specs = tf.estimator.EstimatorSpec(\n",
    "          mode=mode,\n",
    "          predictions=pred_classes,\n",
    "          loss=loss_op,\n",
    "          train_op=train_op,\n",
    "          eval_metric_ops={'accuracy': acc_op, \n",
    "                           'recall': recall,\n",
    "                           'precision': precision\n",
    "                          })\n",
    "\n",
    "        return estim_specs\n",
    "\n",
    "    # Build the Estimator\n",
    "    model = tf.estimator.Estimator(model_fn)\n",
    "\n",
    "    # Train the Model\n",
    "    model.train(input_fn, steps=num_steps)\n",
    "\n",
    "    # Evaluate the Model\n",
    "    # Define the input function for evaluating\n",
    "\n",
    "\n",
    "    input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'images': moocs_test}, y=moocs_label1,\n",
    "        batch_size=batch_size, shuffle=False)\n",
    "    # Use the Estimator 'evaluate' method\n",
    "    results_dic = model.evaluate(input_fn) #\n",
    "    results_dic.update({'f1': (2*((results_dic['precision']*results_dic['recall'])/(results_dic['precision']+results_dic['recall'])))})\n",
    "    results_dic.update({'learning_rate':learning_rate_})\n",
    "    results_dic.update({'neurons':neurons_})\n",
    "    #print (results_dic)\n",
    "    #print ('f1 score: ' + str(2*((results_dic['precision']*results_dic['recall'])/(results_dic['precision']+results_dic['recall']))))\n",
    "    return results_dic\n",
    "#-------------------------------------------------------------------------------------end of map_fun def    \n",
    "#hyperparameter tuning\n",
    "rdd = sc.parallelize([[0.01,40],[0.1,40],[0.3,40],[0.01,31],[0.1,31],[0.3,31]])#[learning rate, neurons per layer]\n",
    "arr = rdd.map(map_fun).collect() #returns an array of dicts\n",
    "\n",
    "for i in arr:\n",
    "    print ('learning rate: ' + str(i['learning_rate']) + ' neurons/layer: ' + str(i['neurons']) + ' accuracy: ' + str(i['accuracy']) + ' f1: ' + str(i['f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
